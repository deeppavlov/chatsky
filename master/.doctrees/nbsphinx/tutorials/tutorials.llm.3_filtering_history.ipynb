{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88ef761",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# LLM: 3. Filtering History\n",
    "\n",
    "If you want to pass only messages that meet specific criteria to the LLM's\n",
    "context, you can use the `filter_func` parameter in `LLMResponse`.\n",
    "\n",
    "This parameter expects an instance of `BaseHistoryFilter` with its\n",
    "abstract `call` method defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a224822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T10:20:40.510961Z",
     "iopub.status.busy": "2025-02-18T10:20:40.510745Z",
     "iopub.status.idle": "2025-02-18T10:20:41.763904Z",
     "shell.execute_reply": "2025-02-18T10:20:41.763047Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing dependencies\n",
    "%pip install -q chatsky[llm]==0.10.0 langchain-openai==0.2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f594c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T10:20:41.766600Z",
     "iopub.status.busy": "2025-02-18T10:20:41.766098Z",
     "iopub.status.idle": "2025-02-18T10:20:43.799106Z",
     "shell.execute_reply": "2025-02-18T10:20:43.798374Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "from chatsky import (\n",
    "    TRANSITIONS,\n",
    "    RESPONSE,\n",
    "    Pipeline,\n",
    "    Transition as Tr,\n",
    "    conditions as cnd,\n",
    "    destinations as dst,\n",
    ")\n",
    "from chatsky.core.message import Message\n",
    "from chatsky.utils.testing import is_interactive_mode\n",
    "from chatsky.llm import LLM_API\n",
    "from chatsky.responses.llm import LLMResponse\n",
    "from chatsky.llm.filters import MessageFilter\n",
    "from chatsky.core.context import Context\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3852562a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T10:20:43.802047Z",
     "iopub.status.busy": "2025-02-18T10:20:43.801402Z",
     "iopub.status.idle": "2025-02-18T10:20:43.863176Z",
     "shell.execute_reply": "2025-02-18T10:20:43.862589Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LLM_API(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key),\n",
    "    system_prompt=\"You are a database assistant and must help your user to \"\n",
    "    \"recover the demanded data from your memory. Act as a note keeper.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfc328",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Imagine having a bot for taking notes that accumulates a large dialogue history.\n",
    "In this example, we will use a simple filtering function to retrieve only the\n",
    "important messages from such a bot, making the context window usage\n",
    "more efficient. Here, we can mark notes with the `#important` tag and then ask\n",
    "the bot to create a summary of all\n",
    "important messages using the `/remind` command.\n",
    "\n",
    "If you want to learn more about filters,\n",
    "refer to the [API reference](../apiref/chatsky.llm.filters.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4375223",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "In order to create a custom filter, we need to inherit either from\n",
    "`BaseHistoryFilter` or `MessageFilter`.\n",
    "\n",
    "`BaseHistoryFilter` is a more complex and more customizable version which allows\n",
    "filtering requests and responses in different ways.\n",
    "\n",
    "In order to use it, inherit from it and define your `call` method\n",
    "that returns and instance of the\n",
    "[Return](../apiref/chatsky.llm.filters.rst#chatsky.llm.filters.Return) enum.\n",
    "\n",
    "`MessageFilter` is a simple version that does not allow for distinction\n",
    "between requests and responses: they are treated the same by the filter.\n",
    "\n",
    "In order to use it, inherit from it and define your `single_message_filter_call`\n",
    "method that returns bool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e2bd81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T10:20:43.865386Z",
     "iopub.status.busy": "2025-02-18T10:20:43.865171Z",
     "iopub.status.idle": "2025-02-18T10:20:43.870172Z",
     "shell.execute_reply": "2025-02-18T10:20:43.869485Z"
    }
   },
   "outputs": [],
   "source": [
    "class FilterImportant(MessageFilter):\n",
    "    def single_message_filter_call(\n",
    "        self, ctx: Context, message: Optional[Message], llm_model_name: str\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        :param ctx: Contex object.\n",
    "        :param message: Either request or response that is being evaluated for\n",
    "            filtering out.\n",
    "        :param llm_model_name: Name of the model that is calling the filter.\n",
    "\n",
    "        :return: Whether message should be included in the history.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            message is not None\n",
    "            and message.text is not None\n",
    "            and \"#important\" in message.text.lower()\n",
    "        ):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b12aa",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Another use case for filters is offered by the built-in\n",
    "[FromModel](../apiref/chatsky.llm.filters.rst#chatsky.llm.filters.FromModel).\n",
    "\n",
    "This filter allows filtering out all messages on turns where response\n",
    "was not generated by the currently used model.\n",
    "\n",
    "Additionally, the `history` parameter in `LLMResponse` allows you to\n",
    "specify the number of dialogue _turns_ that the model will use as history.\n",
    "The default value is `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b707b2c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T10:20:43.872282Z",
     "iopub.status.busy": "2025-02-18T10:20:43.871910Z",
     "iopub.status.idle": "2025-02-18T10:20:43.877985Z",
     "shell.execute_reply": "2025-02-18T10:20:43.877323Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "toy_script = {\n",
    "    \"main_flow\": {\n",
    "        \"start_node\": {\n",
    "            RESPONSE: Message(\"\"),\n",
    "            TRANSITIONS: [Tr(dst=\"greeting_node\", cnd=cnd.ExactMatch(\"Hi\"))],\n",
    "        },\n",
    "        \"greeting_node\": {\n",
    "            RESPONSE: LLMResponse(llm_model_name=\"note_model\", history=0),\n",
    "            TRANSITIONS: [\n",
    "                Tr(dst=\"main_node\", cnd=cnd.ExactMatch(\"Who are you?\"))\n",
    "            ],\n",
    "        },\n",
    "        \"main_node\": {\n",
    "            RESPONSE: Message(\n",
    "                \"Hi! I am your note-taking assistant. \"\n",
    "                \"Just send me your thoughts, and if you need to \"\n",
    "                \"rewind a bit, send `/remind`, and I will provide \"\n",
    "                \"a summary of your #important notes.\"\n",
    "            ),\n",
    "            TRANSITIONS: [\n",
    "                Tr(dst=\"remind_node\", cnd=cnd.ExactMatch(\"/remind\")),\n",
    "                Tr(dst=dst.Current()),\n",
    "            ],\n",
    "        },\n",
    "        \"remind_node\": {\n",
    "            RESPONSE: LLMResponse(\n",
    "                llm_model_name=\"note_model\",\n",
    "                prompt=\"Create a bullet list from all the previous \"\n",
    "                \"messages tagged with #important.\",\n",
    "                history=15,\n",
    "                filter_func=FilterImportant(),\n",
    "            ),\n",
    "            TRANSITIONS: [Tr(dst=\"main_node\")],\n",
    "        },\n",
    "        \"fallback_node\": {\n",
    "            RESPONSE: Message(\"I did not quite understand you...\"),\n",
    "            TRANSITIONS: [Tr(dst=\"main_node\")],\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0204268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T10:20:43.880206Z",
     "iopub.status.busy": "2025-02-18T10:20:43.879733Z",
     "iopub.status.idle": "2025-02-18T10:20:43.884302Z",
     "shell.execute_reply": "2025-02-18T10:20:43.883640Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    toy_script,\n",
    "    start_label=(\"main_flow\", \"start_node\"),\n",
    "    fallback_label=(\"main_flow\", \"fallback_node\"),\n",
    "    models={\"note_model\": model},\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if is_interactive_mode():\n",
    "        pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
