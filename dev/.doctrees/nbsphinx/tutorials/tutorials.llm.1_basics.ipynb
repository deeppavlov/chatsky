{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147a3e37",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# LLM: 1. Basics\n",
    "\n",
    "With Chatsky, you can easily integrate LLM (Large Language Model)\n",
    "invocations into your scripts.\n",
    "This tutorial demonstrates how to use LLMs for generating responses and handling\n",
    "conditional logic in your conversational flows.\n",
    "\n",
    "Chatsky leverages LangChain internally to interface with remote LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e635e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:56:43.667803Z",
     "iopub.status.busy": "2025-02-18T09:56:43.667587Z",
     "iopub.status.idle": "2025-02-18T09:56:44.938609Z",
     "shell.execute_reply": "2025-02-18T09:56:44.937687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing dependencies\n",
    "%pip install -q chatsky[llm]==0.10.0 langchain-openai==0.2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8191fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:56:44.941431Z",
     "iopub.status.busy": "2025-02-18T09:56:44.940974Z",
     "iopub.status.idle": "2025-02-18T09:56:47.280362Z",
     "shell.execute_reply": "2025-02-18T09:56:47.279533Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from chatsky.core.message import Message\n",
    "from chatsky import (\n",
    "    TRANSITIONS,\n",
    "    RESPONSE,\n",
    "    Pipeline,\n",
    "    Transition as Tr,\n",
    "    conditions as cnd,\n",
    "    destinations as dst,\n",
    ")\n",
    "from chatsky.utils.testing import (\n",
    "    is_interactive_mode,\n",
    ")\n",
    "from chatsky.llm import LLM_API\n",
    "from chatsky.responses.llm import LLMResponse\n",
    "from chatsky.conditions.llm import LLMCondition\n",
    "from chatsky.llm.methods import Contains\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a7818",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Model Configuration\n",
    "\n",
    "First, we need to create a model object.\n",
    "\n",
    "LangChain automatically reads environment variables for model configurations,\n",
    "so explicit API key settings aren't always necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47232a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:56:47.283373Z",
     "iopub.status.busy": "2025-02-18T09:56:47.282700Z",
     "iopub.status.idle": "2025-02-18T09:56:47.349110Z",
     "shell.execute_reply": "2025-02-18T09:56:47.348379Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = LLM_API(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key),\n",
    "    system_prompt=\"You are an experienced barista in a local coffe shop. \"\n",
    "    \"Answer your customer's questions about coffee and barista work.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bed0f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The initiated model then needs to be passed to `Pipeline` as such:\n",
    "```python\n",
    "pipeline = Pipeline(\n",
    "    ...\n",
    "    models={\n",
    "        \"my_model_name\": model\n",
    "    }\n",
    ")\n",
    "```\n",
    "Model name is used to reference the model config in the LLM script functions.\n",
    "\n",
    "You can also make multiple models and pass them together in the `models`\n",
    "dictionary. This allows using different system prompts and/or\n",
    "model configs in the same script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b9748",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "As you can see in this script, you can pass an additional prompt to the LLM.\n",
    "We will cover that more thoroughly in the\n",
    "[next tutorial](../tutorials/tutorials.llm.2_prompt_usage.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc46082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:56:47.352021Z",
     "iopub.status.busy": "2025-02-18T09:56:47.351550Z",
     "iopub.status.idle": "2025-02-18T09:56:47.359151Z",
     "shell.execute_reply": "2025-02-18T09:56:47.358581Z"
    }
   },
   "outputs": [],
   "source": [
    "toy_script = {\n",
    "    \"main_flow\": {\n",
    "        \"start_node\": {\n",
    "            RESPONSE: \"\",\n",
    "            TRANSITIONS: [Tr(dst=\"greeting_node\", cnd=cnd.ExactMatch(\"Hi\"))],\n",
    "        },\n",
    "        \"greeting_node\": {\n",
    "            RESPONSE: LLMResponse(llm_model_name=\"barista_model\", history=0),\n",
    "            TRANSITIONS: [\n",
    "                Tr(dst=\"main_node\", cnd=cnd.ExactMatch(\"Who are you?\"))\n",
    "            ],\n",
    "        },\n",
    "        \"main_node\": {\n",
    "            RESPONSE: LLMResponse(llm_model_name=\"barista_model\"),\n",
    "            TRANSITIONS: [\n",
    "                Tr(\n",
    "                    dst=\"latte_art_node\",\n",
    "                    cnd=cnd.ExactMatch(\"I want to tell you about latte art.\"),\n",
    "                ),\n",
    "                Tr(\n",
    "                    dst=\"boss_node\",\n",
    "                    cnd=LLMCondition(\n",
    "                        llm_model_name=\"barista_model\",\n",
    "                        prompt=\"Return TRUE if the customer insists \"\n",
    "                        \"they are your boss, and FALSE otherwise. \"\n",
    "                        \"Only ONE word must be in the output.\",\n",
    "                        method=Contains(pattern=\"TRUE\"),\n",
    "                    ),\n",
    "                ),\n",
    "                Tr(dst=dst.Current()),\n",
    "            ],\n",
    "        },\n",
    "        \"boss_node\": {\n",
    "            RESPONSE: Message(\"You are my boss.\"),\n",
    "            TRANSITIONS: [\n",
    "                Tr(dst=\"main_node\"),\n",
    "            ],\n",
    "        },\n",
    "        \"latte_art_node\": {\n",
    "            RESPONSE: LLMResponse(\n",
    "                llm_model_name=\"barista_model\",\n",
    "                prompt=\"PROMPT: pretend that you have never heard about latte \"\n",
    "                \"art before and DO NOT answer the following questions. \"\n",
    "                \"Instead ask a person about it.\",\n",
    "            ),\n",
    "            TRANSITIONS: [\n",
    "                Tr(dst=\"main_node\", cnd=cnd.ExactMatch(\"Ok, goodbye.\")),\n",
    "                Tr(dst=dst.Current()),\n",
    "            ],\n",
    "        },\n",
    "        \"fallback_node\": {\n",
    "            RESPONSE: Message(\"I didn't quite understand you...\"),\n",
    "            TRANSITIONS: [Tr(dst=\"main_node\")],\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd260ad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:56:47.361330Z",
     "iopub.status.busy": "2025-02-18T09:56:47.360919Z",
     "iopub.status.idle": "2025-02-18T09:56:47.365591Z",
     "shell.execute_reply": "2025-02-18T09:56:47.364928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Register your model in the pipeline's `models` field using the same key\n",
    "# referenced as `llm_model_name` in your script nodes\n",
    "pipeline = Pipeline(\n",
    "    toy_script,\n",
    "    start_label=(\"main_flow\", \"start_node\"),\n",
    "    fallback_label=(\"main_flow\", \"fallback_node\"),\n",
    "    models={\"barista_model\": model},\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if is_interactive_mode():\n",
    "        pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
